{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 블록 중 최대 토큰 개수 (max token length): 146\n",
      "전체 블록 중 최대 생성토큰 개수 (gen_length): 102\n",
      "전체 블록 중 최대 토큰 결과 (max_sentence): 1) Brain problems are a pain, but they are not the answer. 2) Sitting down is not the answer, as the pain can be caused by standing. 3) Fatness is not the answer, as there is no mention of eating. 4) Laughter is not the answer, as it is not mentioned. 5) The answer is a pain in the head. Headaches are a common consequence of sitting close to a TV. Therefore, the answer is headache (D).\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# GPT-2 토크나이저 초기화\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B\")\n",
    "\n",
    "def extract_answer_token_length(text):\n",
    "    \"\"\"\n",
    "    주어진 텍스트에서 \"A:\" 이후의 텍스트를 추출하고,\n",
    "    해당 부분의 토큰 길이를 반환합니다.\n",
    "    \"\"\"\n",
    "    match = re.search(r\"A:\\s*(.*)\", text, re.DOTALL)\n",
    "    if match:\n",
    "        answer_text = match.group(1).strip()\n",
    "        tokens = tokenizer.tokenize(answer_text)\n",
    "        token_count = len(tokens)\n",
    "        return answer_text, token_count\n",
    "    else:\n",
    "        return None, 0\n",
    "\n",
    "# 텍스트 파일 읽기 (예시 파일명: data.txt)\n",
    "with open(\"cqa/cqa_vanilla_10/cqa_vanilla_10_0/correct_data.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    content = file.read()\n",
    "\n",
    "# 각 데이터 블록은 \"<|end_of_text|>\" 로 구분되어 있다고 가정\n",
    "blocks = content.split(\"<|end_of_text|>\")\n",
    "\n",
    "max_token_length = 0  # 전체 블록 중 최대 토큰 개수를 저장할 변수\n",
    "max_sentence=''\n",
    "gen_length=0\n",
    "\n",
    "for i, block in enumerate(blocks):\n",
    "    # \"A:\" 이후 텍스트와 토큰 수 계산\n",
    "    answer_text, answer_token_count = extract_answer_token_length(block)\n",
    "    # 블록 전체의 토큰 수 계산\n",
    "    block_tokens = tokenizer.tokenize(block)\n",
    "    block_token_count = len(block_tokens)    \n",
    "    \n",
    "    # 최대 토큰 개수 갱신\n",
    "    if block_token_count > max_token_length:\n",
    "        max_token_length = block_token_count\n",
    "        gen_length=answer_token_count\n",
    "        max_sentence=answer_text\n",
    "\n",
    "print(\"전체 블록 중 최대 토큰 개수 (max token length):\", max_token_length)\n",
    "print(\"전체 블록 중 최대 생성토큰 개수 (gen_length):\", gen_length)\n",
    "print(\"전체 블록 중 최대 토큰 결과 (max_sentence):\", max_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
